{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9456d9-ae0a-47bd-8a02-ff9e00e32053",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning, Missing Observations and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d8e74-4884-49e7-a965-4b472f88b625",
   "metadata": {},
   "source": [
    "This project applies a fully structured preprocessing pipeline—a **quality-assurance framework** designed to stabilise messy Airbnb listing data, protect data integrity, and enable reliable, leakage-free modelling. Each stage enforces strict data standards, converts raw inputs into consistent formats, and produces a clean, reproducible feature set ready for machine-learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Protects Data Quality**\n",
    "\n",
    "Handles inconsistent formats, missing values, skewed distributions, and unstructured text by:\n",
    "\n",
    "* enforcing correct types for dates, rates, and numerics\n",
    "* cleaning all text fields\n",
    "* parsing lists (amenities, verifications)\n",
    "* standardising geographic fields\n",
    "* extracting structured review, bathroom, and host features\n",
    "\n",
    "This prevents bad raw data from contaminating later steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Ensures Train/Test Consistency**\n",
    "\n",
    "All engineered features follow the same rules across both datasets:\n",
    "\n",
    "* Top-N amenities learned from **train only**\n",
    "* One-hot encoding aligned to train columns\n",
    "* TargetEncoder fitted only on train tiers\n",
    "* Neighbourhood density mapped from train → test\n",
    "* Outlier caps learned from train quantiles\n",
    "\n",
    "Guarantees identical feature space and fair evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Prevents Data Leakage**\n",
    "\n",
    "The pipeline avoids target contamination by:\n",
    "\n",
    "* computing imputation stats on **training data only**\n",
    "* learning hierarchical mappings from train only\n",
    "* restricting price-tier encodings to train\n",
    "* applying transformations without exposing test distribution\n",
    "\n",
    "This preserves model validity and generalisation.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Modular and Reproducible**\n",
    "\n",
    "Clear stage boundaries:\n",
    "\n",
    "* Stage 1: Cleaning\n",
    "* Stage 2: Imputation\n",
    "* Stage 3: Encoding\n",
    "* Stage 4: Feature Engineering\n",
    "* Stage 5: Outlier Transformation\n",
    "\n",
    "This modular structure makes the workflow easy to debug, transparent, and reproducible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb1e6f-d53b-44eb-8af4-45efd0d07b82",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f457947-1352-483e-beb5-e9dba8c542da",
   "metadata": {},
   "source": [
    "At first, I import all the libraries needed for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "babb0684-2c3d-4348-953c-42383f88e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import re                           # Regular expressions for string cleaning\n",
    "import time                         # Timing operations (e.g., geocoding loops)\n",
    "import warnings                     # To suppress irrelevant warnings\n",
    "warnings.filterwarnings('ignore')   # Silence warnings for cleaner output/logs\n",
    "\n",
    "# --- Data Handling & Math ---\n",
    "import pandas as pd                 # Core data manipulation library\n",
    "import numpy as np                  # Numerical computing and vectorised operations\n",
    "\n",
    "# --- Sentiment Analysis ---\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  \n",
    "# Used to compute sentiment scores on text fields such as description or host_about\n",
    "\n",
    "# --- Geocoding & Geospatial Processing ---\n",
    "from geopy.geocoders import Nominatim     # For converting addresses → latitude/longitude\n",
    "from geopy.point import Point             # Geopy coordinate object\n",
    "import geopandas as gpd                   # Geospatial operations on GeoDataFrames\n",
    "import osmnx as ox                        # OpenStreetMap (OSM) data download + routing\n",
    "from shapely.geometry import Point         # Shapely geometric point objects\n",
    "from sklearn.neighbors import BallTree     # Fast nearest-neighbour spatial search\n",
    "\n",
    "# --- Feature Engineering / Encoding ---\n",
    "from category_encoders import TargetEncoder  \n",
    "# Encoding categorical values based on target mean (helps models handle high-cardinality categories)\n",
    "\n",
    "# --- Utilities ---\n",
    "from tqdm import tqdm                      # Progress bars for long loops (e.g., geocoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783888c6-edf0-40f4-96a7-4a7e9b81a510",
   "metadata": {},
   "source": [
    "These libraries handle:\n",
    "- data cleaning and manipulation (pandas, numpy)\n",
    "- sentiment analysis for text features (VADER)\n",
    "- geocoding and spatial processing (geopy, geopandas, osmnx, shapely, BallTree)\n",
    "- feature encoding for high-cardinality categories (TargetEncoder)\n",
    "- progress monitoring for long loops (tqdm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58692f83-d8bd-41c3-a22e-e596f86861d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim  # Import geocoder\n",
    "# from geopy.point import Point  # Import geographic point object\n",
    "\n",
    "# geolocator = Nominatim(user_agent='myGeocoder')  # Initialize geolocator with user agent\n",
    "\n",
    "# def get_suburb_info(row):  # Define function to extract suburb or similar info\n",
    "#     lat = row['latitude']\n",
    "#     long = row['longitude']\n",
    "#     location = geolocator.reverse(Point(lat, long), exactly_one=True, timeout=5)  # Reverse geocode\n",
    "#     if location:\n",
    "#         address = location.raw.get('address')  # Extract address dictionary\n",
    "#         suburb = address.get('suburb')\n",
    "#         town = address.get('town')\n",
    "#         city_district = address.get('city_district')\n",
    "#         village = address.get('village')\n",
    "#         hamlet = address.get('hamlet')\n",
    "#         if suburb: \n",
    "#             return suburb\n",
    "#         elif town:\n",
    "#             return town\n",
    "#         elif city_district:\n",
    "#             return city_district\n",
    "#         elif village:\n",
    "#             return village\n",
    "#         elif hamlet:\n",
    "#             return hamlet\n",
    "#     return 0  # Return 0 if no valid result found\n",
    "\n",
    "# df['suburb'] = df.apply(get_suburb_info, axis=1, result_type='expand')  # Apply function to all rows\n",
    "\n",
    "# def get_council_info(row):  # Define function to extract municipality (council)\n",
    "#     lat = row['latitude']\n",
    "#     long = row['longitude']\n",
    "#     location = geolocator.reverse(Point(lat, long), exactly_one=True, timeout=5)\n",
    "#     if location:\n",
    "#         address = location.raw.get('address')\n",
    "#         municipality = address.get('municipality')  # Extract municipality field\n",
    "#         return municipality\n",
    "#     else:\n",
    "#         return None  # Return None if not found\n",
    "\n",
    "# df['municipality'] = df.apply(get_council_info, axis=1, result_type='expand')  # Apply function\n",
    "\n",
    "# df_lookup = df[['latitude', 'longitude', 'suburb', 'municipality']].drop_duplicates()  # Keep only unique geocoded entries\n",
    "# df_lookup.to_csv(\"suburb_municipality_lookup.csv\", index=False)  # Save to CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a641d-3c3e-4eb7-ac63-3fc21b49902c",
   "metadata": {},
   "source": [
    "A reverse-geocoding module was implemented using Geopy to enrich each listing with accurate suburb and municipality information derived directly from latitude and longitude, replacing inconsistent Airbnb-provided location fields. A unique lookup table was generated to ensure reproducible, non-leaky, and efficient geographic enrichment across both the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cfb4e-7c76-4e18-8ca9-0df38033e9c2",
   "metadata": {},
   "source": [
    "## Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3dc7f4e-2aab-4618-b777-dfea7ba4af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_content(text):\n",
    "    \"\"\"\n",
    "    Removes HTML, invisible characters, and collapses spaces.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\\\n|\\\\r|\\\\t|&nbsp;|nbsp|br|\\\\xa0|&amp;', ' ', text)  # Invisible chars/entities\n",
    "    text = re.sub(r'\\s+', ' ', text)   # Collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_amenities_string(x):\n",
    "    \"\"\"\n",
    "    Parses amenity strings into clean lists.\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    x = re.sub(r'[\\[\\]{}\"\\']', '', x)  # Remove brackets and quotes\n",
    "    return [a.strip().lower() for a in x.split(',') if a.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024f5b7-6424-45b3-ba23-b99d371f9816",
   "metadata": {},
   "source": [
    "In this section, I define small helper functions used throughout the pipeline.\n",
    "\n",
    "**`clean_text_content():`**\n",
    "- Cleans raw text fields by removing HTML tags, invisible characters, and unnecessary whitespace.\n",
    "- Ensures all text data is standardised before sentiment analysis or feature extraction.\n",
    "\n",
    "**`clean_amenities_string():`**\n",
    "- Converts the raw 'amenities' string into a clean list of individual items.\n",
    "- Removes brackets, quotes, and extra spaces, and lowercases everything.\n",
    "- Prepares the amenities field for feature engineering steps later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab7a4ff-f6f0-4bcd-b1c2-4b63528c593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_price_and_rates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns: 'price', 'host_response_rate', 'host_acceptance_rate'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Price\n",
    "    if 'price' in df.columns:\n",
    "        df['price'] = df['price'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "        df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "    # Host rates\n",
    "    for col in ['host_response_rate', 'host_acceptance_rate']:\n",
    "        if col in df.columns:\n",
    "            df[col] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.replace('%', '', regex=False)\n",
    "                .replace('nan', np.nan)\n",
    "            )\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col] / 100.0   # convert to proportion\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _process_bathrooms(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns: 'bathrooms', 'bath_type', 'bath_type_cleaned'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'bathrooms' not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # Bath type classification from original string\n",
    "    df['bath_type'] = df['bathrooms'].astype(str).apply(\n",
    "        lambda x: 'Shared' if 'shared' in x.lower() else\n",
    "                  ('Private' if 'private' in x.lower() else 'Normal')\n",
    "    )\n",
    "\n",
    "    df['bath_type_cleaned'] = (\n",
    "        df['bath_type']\n",
    "        .fillna('unknown')\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "    # Bathroom numeric extraction\n",
    "    df['bathrooms'] = df['bathrooms'].astype(str).str.replace(\n",
    "        'Half', '0.5', case=False, regex=False\n",
    "    )\n",
    "    df['bathrooms'] = df['bathrooms'].str.extract(r'(\\d+\\.?\\d*)')[0]\n",
    "    df['bathrooms'] = pd.to_numeric(df['bathrooms'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _process_host_verifications(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns:\n",
    "      'host_verifications', 'verifications_list',\n",
    "      'email_verified', 'phone_verified', 'work_email_verified', 'num_verifications'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'host_verifications' not in df.columns:\n",
    "        return df\n",
    "\n",
    "    df['host_verifications'] = df['host_verifications'].fillna('').astype(str)\n",
    "\n",
    "    df['verifications_list'] = (\n",
    "        df['host_verifications']\n",
    "        .str.replace(r\"[\\[\\]']\", '', regex=True)\n",
    "        .str.lower()\n",
    "        .str.split(',')\n",
    "    )\n",
    "\n",
    "    df['verifications_list'] = df['verifications_list'].apply(\n",
    "        lambda x: [i.strip() for i in x if i.strip()]\n",
    "    )\n",
    "\n",
    "    df['email_verified'] = df['verifications_list'].apply(lambda x: int('email' in x))\n",
    "    df['phone_verified'] = df['verifications_list'].apply(lambda x: int('phone' in x))\n",
    "    df['work_email_verified'] = df['verifications_list'].apply(lambda x: int('work_email' in x))\n",
    "    df['num_verifications'] = df['verifications_list'].apply(len)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _process_amenities(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns: 'amenities', 'amenities_list'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'amenities' not in df.columns:\n",
    "        return df\n",
    "\n",
    "    df['amenities'] = df['amenities'].fillna('').astype(str)\n",
    "    df['amenities_list'] = df['amenities'].apply(clean_amenities_string)\n",
    "\n",
    "    # No counts/frequency here – that belongs to feature engineering stage.\n",
    "    return df\n",
    "\n",
    "\n",
    "def _process_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns: 'host_since', 'last_review'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'host_since' in df.columns:\n",
    "        df['host_since'] = pd.to_datetime(df['host_since'], errors='coerce')\n",
    "\n",
    "    if 'last_review' in df.columns:\n",
    "        df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _process_text_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns: 'name', 'description', 'host_about', 'neighborhood_overview'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    text_cols = ['name', 'description', 'host_about', 'neighborhood_overview']\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(clean_text_content)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _merge_geolocation(df: pd.DataFrame,\n",
    "                       lookup_csv_path: str = \"suburb_municipality_lookup.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Owns: 'suburb', 'municipality'\n",
    "    (Assumes an 'ID' column exists in both df and the lookup CSV)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "    try:\n",
    "        lookup_df = pd.read_csv(lookup_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: '{lookup_csv_path}' not found. Skipping geolocation merge.\")\n",
    "        return df\n",
    "\n",
    "    if 'ID' in df.columns and 'ID' in lookup_df.columns:\n",
    "        df = df.merge(\n",
    "            lookup_df[['ID', 'suburb', 'municipality']],\n",
    "            on='ID',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        if 'municipality' in df.columns:\n",
    "            df['municipality'] = (\n",
    "                df['municipality']\n",
    "                .astype(str)\n",
    "                .str.replace('City of ', '', regex=False)\n",
    "                .str.replace('Shire of ', '', regex=False)\n",
    "                .replace('nan', np.nan)\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58454e77-3301-4719-83d2-15158f62cdbb",
   "metadata": {},
   "source": [
    "**1. `_clean_price_and_rates(df)`**\n",
    "\n",
    "- This function standardises the price and host percentage rate fields.\n",
    "\n",
    "- Cleans the price column by removing $ and commas, then converts it to numeric.\n",
    "\n",
    "- Cleans host_response_rate and host_acceptance_rate by removing %, handling \"nan\", converting to numbers, and dividing by 100 to convert percentages into proportions (e.g., 85% → 0.85).\n",
    "\n",
    "- Ensures all rate-related columns are consistent and model-ready.\n",
    "\n",
    "---\n",
    "\n",
    "**2. `_process_bathrooms(df)`**\n",
    "\n",
    "- This function extracts clean bathroom information and derives bathroom-type features.\n",
    "\n",
    "- Creates a bath_type column by detecting keywords such as “shared” or “private”.\n",
    "\n",
    "- Normalises bath type into bath_type_cleaned.\n",
    "\n",
    "- Converts bathroom descriptions like \"Half-bath\" into numeric values (e.g., “Half” → 0.5).\n",
    "\n",
    "- Extracts numeric quantities using regex and converts them to numeric type.\n",
    "\n",
    "---\n",
    "\n",
    "**3. `_process_host_verifications(df)`**\n",
    "\n",
    "- This function converts the host verification string into structured features.\n",
    "\n",
    "- Parses the raw host_verifications text into a clean list.\n",
    "\n",
    "- Generates binary flags: email_verified, phone_verified, and work_email_verified.\n",
    "\n",
    "- Creates num_verifications, counting how many verifications a host has.\n",
    "\n",
    "- Helps models understand host trustworthiness using structured features.\n",
    "\n",
    "--- \n",
    "\n",
    "**4. `_process_amenities(df)`**\n",
    "\n",
    "- This function parses the raw amenities field.\n",
    "\n",
    "- Converts the amenities string into a clean Python list using the earlier clean_amenities_string() helper.\n",
    "\n",
    "---\n",
    "\n",
    "**5. `_process_dates(df)`**\n",
    "\n",
    "- This function converts important date fields into datetime objects.\n",
    "\n",
    "- Parses host_since and last_review into proper datetime format.\n",
    "\n",
    "- Prepares the columns for later calculations such as host experience or review recency.\n",
    "\n",
    "---\n",
    "\n",
    "**6. `_process_text_columns(df)`**\n",
    "\n",
    "- This function cleans important text fields for later NLP/sentiment processing.\n",
    "\n",
    "- Applies clean_text_content() to name, description, host_about, and neighborhood_overview.\n",
    "\n",
    "- Removes HTML, invisible characters, and normalises spacing.\n",
    "\n",
    "- Ensures clean text for sentiment analysis or keyword extraction.\n",
    "\n",
    "---\n",
    "\n",
    "**7. `_merge_geolocation(df)`**\n",
    "\n",
    "- This function links the dataset with an external lookup table to enrich location fields.\n",
    "\n",
    "- Converts latitude and longitude into numeric format.\n",
    "\n",
    "- Merges a lookup CSV using ID (if available) to bring in suburb and municipality.\n",
    "\n",
    "- Cleans municipality names by removing “City of” and “Shire of”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab6b812-3cb7-469f-b823-23286d52224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1_cleaning(df: pd.DataFrame,\n",
    "                    lookup_csv_path: str = \"suburb_municipality_lookup.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Stage 1 cleaning: calls non-overlapping helpers.\n",
    "\n",
    "    Column ownership summary:\n",
    "      - _clean_price_and_rates:      price, host_response_rate, host_acceptance_rate\n",
    "      - _process_bathrooms:          bathrooms, bath_type, bath_type_cleaned\n",
    "      - _process_host_verifications: host_verifications, verifications_list,\n",
    "                                     email_verified, phone_verified,\n",
    "                                     work_email_verified, num_verifications\n",
    "      - _process_amenities:          amenities, amenities_list\n",
    "      - _process_dates:              host_since, last_review\n",
    "      - _process_text_columns:       name, description, host_about, neighborhood_overview\n",
    "      - _merge_geolocation:          suburb, municipality\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    df_clean = _clean_price_and_rates(df_clean)\n",
    "    df_clean = _process_bathrooms(df_clean)\n",
    "    df_clean = _process_host_verifications(df_clean)\n",
    "    df_clean = _process_amenities(df_clean)\n",
    "    df_clean = _process_dates(df_clean)\n",
    "    df_clean = _process_text_columns(df_clean)\n",
    "    df_clean = _merge_geolocation(df_clean, lookup_csv_path=lookup_csv_path)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6cc47-50fe-472e-b63b-160b6dea2804",
   "metadata": {},
   "source": [
    "**`step_1_cleaning`**\n",
    "\n",
    "This function coordinates all individual cleaning helpers and ensures Stage 1 Cleaning is applied in the correct order. This step does not include its own cleaning logic; instead, it sequentially calls each specialised helper function. Every helper is responsible for specific columns, ensuring there is no overlap or accidental reprocessing.<br>\n",
    "\n",
    "Functions called in this stage:\n",
    "\n",
    "- `_clean_price_and_rates()` → cleans price and host rate percentages\n",
    "\n",
    "- `_process_bathrooms()` → extracts bathroom counts and bathroom type\n",
    "\n",
    "- `_process_host_verifications()` → parses verification lists and creates binary flags\n",
    "\n",
    "- `_process_amenities()` → converts amenities into clean lists\n",
    "\n",
    "- `_process_dates()` → converts date fields to datetime\n",
    "\n",
    "- `_process_text_columns()` → cleans text fields for NLP\n",
    "\n",
    "- `_merge_geolocation()` → merges in suburb/municipality lookup information\n",
    "\n",
    "---\n",
    "\n",
    "This function separates responsibilities to prevent column conflicts, applies consistent cleaning across both train and test datasets, and maintains a modular, easy-to-debug preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a890684-7282-48f8-a569-13987f7be831",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf2569-bd2a-46e8-b9cd-480164646a2b",
   "metadata": {},
   "source": [
    "These functions improve data completeness by inferring missing types, location, and availability from existing signals in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b55194-0f9b-4009-964b-ebf6f8fe298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_missing_types_logic(df):\n",
    "    \"\"\"\n",
    "    Infers property_type and room_type from the 'description' text \n",
    "    using keyword matching (Regex).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- A. Infer Property Type ---\n",
    "    if 'description' in df.columns and 'property_type' in df.columns:\n",
    "        desc = df['description'].fillna('').str.lower()\n",
    "        conditions = [\n",
    "            desc.str.contains('villa'),\n",
    "            desc.str.contains('cabin|cottage|barn|hut|yurt|chalet'),\n",
    "            desc.str.contains('apartment|flat|unit|condo'),\n",
    "            desc.str.contains('studio|loft|tiny home'),\n",
    "            desc.str.contains('farm|nature lodge'),\n",
    "            desc.str.contains('tent|camper|rv|boat|train|tower'),\n",
    "            desc.str.contains('entire home|entire house'),\n",
    "            desc.str.contains('private room'),\n",
    "            desc.str.contains('shared room|bunk'),\n",
    "        ]\n",
    "        choices = [\n",
    "            'Entire villa', 'Entire cabin', 'Entire rental unit', 'Entire studio',\n",
    "            'Farm stay', 'Camper/RV', 'Entire home', 'Private room in home',\n",
    "            'Shared room in rental unit'\n",
    "        ]\n",
    "        # Infer and Fill\n",
    "        inferred_prop = np.select(conditions, choices, default=None)\n",
    "        # Only fill missing values\n",
    "        df['property_type'] = df['property_type'].fillna(pd.Series(inferred_prop))\n",
    "\n",
    "    # --- B. Infer Room Type ---\n",
    "    if 'description' in df.columns and 'room_type' in df.columns:\n",
    "        desc = df['description'].fillna('').str.lower()\n",
    "        conditions_room = [\n",
    "            desc.str.contains(r'\\b(entire|whole)\\b.*\\b(home|apartment|house|flat|unit)\\b', regex=True),\n",
    "            desc.str.contains(r'\\b(private room|private bedroom|own room)\\b', regex=True),\n",
    "            desc.str.contains(r'\\b(shared room|bunk bed|dormitory|shared)\\b', regex=True),\n",
    "            desc.str.contains(r'\\b(hotel|suite|serviced apartment|aparthotel)\\b', regex=True),\n",
    "        ]\n",
    "        choices_room = ['Entire home/apt', 'Private room', 'Shared room', 'Hotel room']\n",
    "        \n",
    "        # Infer and Fill\n",
    "        inferred_room = np.select(conditions_room, choices_room, default='Unknown')\n",
    "        inferred_series = pd.Series(inferred_room).replace('Unknown', np.nan)\n",
    "        df['room_type'] = df['room_type'].fillna(inferred_series)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660bfc3-d273-45de-bcb7-af2cc266610d",
   "metadata": {},
   "source": [
    "**`infer_missing_types_logic(df)`**\n",
    "\n",
    "This function infers missing `property_type` and `room_type` values from the `description` text using keyword-based rules (regex-style matching).\n",
    "\n",
    "* For **`property_type`**, it scans the description for words like *villa*, *cabin*, *apartment*, *studio*, *farm*, *camper/RV*, *private room*, or *shared room*, and maps them to consistent property type labels.\n",
    "* It only fills values where `property_type` is missing, so existing values are not overwritten.\n",
    "* For **`room_type`**, it looks for phrases like *entire home/apartment*, *private room*, *shared room*, or *hotel/suite*, and infers a standardised room type.\n",
    "* Inferred values are used only to fill missing `room_type`, keeping any original entries intact.\n",
    "\n",
    "Overall, this function enriches incomplete listings by using textual clues in the description to recover likely property and room types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ce7fba-ecdd-4023-bfbd-a3e98ae208ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_neighbourhood_from_geo(df):\n",
    "    \"\"\"\n",
    "    Reproduce old behaviour:\n",
    "    - neighbourhood = suburb\n",
    "    - neighbourhood_cleansed filled from municipality when missing\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'suburb' in df.columns:\n",
    "        # df['neighbourhood'] = df['suburb']\n",
    "        # To match exactly, do the same:\n",
    "        df['neighbourhood'] = df['suburb']\n",
    "\n",
    "    if 'municipality' in df.columns and 'neighbourhood_cleansed' in df.columns:\n",
    "        df['neighbourhood_cleansed'] = df['neighbourhood_cleansed'].fillna(df['municipality'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794a527-acbf-4b36-9f8e-5edb617585ee",
   "metadata": {},
   "source": [
    "**`impute_neighbourhood_from_geo(df)`**\n",
    "\n",
    "This function reconstructs the old behaviour for neighbourhood-related fields using geolocation features.\n",
    "\n",
    "* Sets `neighbourhood` equal to `suburb`, matching the original pipeline logic.\n",
    "* Fills missing `neighbourhood_cleansed` values using `municipality` where available.\n",
    "\n",
    "This keeps the location-related variables consistent with the previous implementation while leveraging suburb and municipality information to reduce missingness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87148188-4524-4837-95d4-5e5ce4e1b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_availability_logic(df):\n",
    "    \"\"\"\n",
    "    Fills 'availability_365' using 'availability_90' * 4 if 365 is missing.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if 'availability_365' in df.columns and 'availability_90' in df.columns:\n",
    "        mask = df['availability_365'].isna() & df['availability_90'].notna()\n",
    "        df.loc[mask, 'availability_365'] = df.loc[mask, 'availability_90'] * 4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d04bb-7a68-4365-81a1-ad17435d6b0b",
   "metadata": {},
   "source": [
    "**`impute_availability_logic(df)`**\n",
    "\n",
    "This function imputes missing `availability_365` values using `availability_90`.\n",
    "\n",
    "* Where `availability_365` is missing but `availability_90` is present, it sets:\n",
    "  **`availability_365 = availability_90 * 4`**\n",
    "* This assumes that the 90-day availability pattern roughly scales across the full year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26843878-5bad-481f-96cc-62e761c0a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_2_imputation(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Main Imputation Orchestrator.\n",
    "    1. Fits stats (Mode/Median/Skew) on TRAIN only.\n",
    "    2. Transforms both TRAIN and TEST using those stats.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 2: Processing missing values ---\")\n",
    "    \n",
    "    # Create copies to avoid SettingWithCopy warnings\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP A: LOGIC-BASED INFERENCE (Row-by-Row)\n",
    "    # ---------------------------------------------------------\n",
    "    # Apply to each dataset and write back\n",
    "    train_df = infer_missing_types_logic(train_df)\n",
    "    train_df = impute_availability_logic(train_df)\n",
    "    train_df = impute_neighbourhood_from_geo(train_df)\n",
    "\n",
    "    test_df = infer_missing_types_logic(test_df)\n",
    "    test_df = impute_availability_logic(test_df)\n",
    "    test_df = impute_neighbourhood_from_geo(test_df)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP B: TEXT PLACEHOLDERS\n",
    "    # ---------------------------------------------------------\n",
    "    text_placeholders = {\n",
    "        'name': 'Unnamed Listing',\n",
    "        'description': 'No description provided',\n",
    "        'neighborhood_overview': 'No neighborhood overview',\n",
    "        'host_about': 'No host description'\n",
    "    }\n",
    "    for col, fallback in text_placeholders.items():\n",
    "        if col in train_df.columns: train_df[col] = train_df[col].fillna(fallback)\n",
    "        if col in test_df.columns: test_df[col] = test_df[col].fillna(fallback)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP C: FIT (LEARN) STATISTICS FROM TRAINING DATA\n",
    "    # ---------------------------------------------------------\n",
    "    stats_map = {}\n",
    "    \n",
    "    # 1. Categorical Modes (Learn from Train)\n",
    "    cat_cols = ['host_location', 'host_neighbourhood', 'host_response_time', 'host_is_superhost']\n",
    "    for col in cat_cols:\n",
    "        if col in train_df.columns:\n",
    "            stats_map[col] = train_df[col].mode().iloc[0]\n",
    "\n",
    "    # 2. Numerical Skew-Aware Stats (Learn from Train)\n",
    "    num_cols = [\n",
    "        'host_response_rate', 'host_acceptance_rate', 'bathrooms', 'bedrooms', 'beds',\n",
    "        'minimum_minimum_nights', 'maximum_maximum_nights', 'review_scores_rating', \n",
    "        'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', \n",
    "        'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "        'reviews_per_month'\n",
    "    ]\n",
    "    for col in num_cols:\n",
    "        if col in train_df.columns:\n",
    "            skewness = train_df[col].skew()\n",
    "            # If symmetric (skew < 0.5), use Mean. Else use Median.\n",
    "            if abs(skewness) < 0.5:\n",
    "                stats_map[col] = train_df[col].mean()\n",
    "            else:\n",
    "                stats_map[col] = train_df[col].median()\n",
    "\n",
    "    # 3. Hierarchical Map (Property Type -> Room Type Mode)\n",
    "    # Learn which room_type is most common for each property_type in TRAIN\n",
    "    if 'property_type' in train_df.columns and 'room_type' in train_df.columns:\n",
    "        room_mode_map = (\n",
    "            train_df.groupby('property_type')['room_type']\n",
    "            .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "            .to_dict()\n",
    "        )\n",
    "        # Also get global defaults from TRAIN\n",
    "        global_prop_mode = train_df['property_type'].mode()[0]\n",
    "        global_room_mode = train_df['room_type'].mode()[0]\n",
    "    else:\n",
    "        room_mode_map = {}\n",
    "        global_prop_mode, global_room_mode = \"Entire rental unit\", \"Entire home/apt\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP D: TRANSFORM (APPLY) TO BOTH DATASETS\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    def apply_imputation(df):\n",
    "        # 1. Apply Simple Stats (Cat & Num) using the stats_map learned from Train\n",
    "        for col, value in stats_map.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(value)\n",
    "        \n",
    "        # 2. Apply Hierarchical (Prop -> Room)\n",
    "        if 'room_type' in df.columns:\n",
    "            def fill_room(row):\n",
    "                if pd.isna(row['room_type']):\n",
    "                    return room_mode_map.get(row['property_type'], np.nan)\n",
    "                return row['room_type']\n",
    "            df['room_type'] = df.apply(fill_room, axis=1)\n",
    "        \n",
    "        # 3. Final Fallback for Prop/Room (Global Modes from Train)\n",
    "        if 'property_type' in df.columns:\n",
    "            df['property_type'] = df['property_type'].fillna(global_prop_mode)\n",
    "        if 'room_type' in df.columns:\n",
    "            df['room_type'] = df['room_type'].fillna(global_room_mode)\n",
    "\n",
    "        # 4. Date Imputation (Sort -> FFill/BFill)\n",
    "        # Impute dates internally within the dataset (no cross-dataset leaking)\n",
    "        if 'ID' in df.columns:\n",
    "            df = df.sort_values(by='ID')\n",
    "            if 'first_review' in df.columns:\n",
    "                df['first_review'] = df['first_review'].fillna(method='ffill')\n",
    "            if 'last_review' in df.columns:\n",
    "                df['last_review'] = df['last_review'].fillna(method='bfill')\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    # Apply the logic\n",
    "    train_filled = apply_imputation(train_df)\n",
    "    test_filled = apply_imputation(test_df)\n",
    "    \n",
    "    print(\"--- Stage 2: Imputation Complete ---\")\n",
    "    return train_filled, test_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd61ccc-1ee4-4d36-a1a6-c2f38bd1acc1",
   "metadata": {},
   "source": [
    "**`stage_2_imputation`**\n",
    "\n",
    "This function manages all missing-value handling for both the training and test datasets.\n",
    "It learns imputation rules **only from the training set**, then applies them consistently to both datasets to avoid data leakage.\n",
    "\n",
    "**1. Logic-based inference (row-level fixes)**\n",
    "\n",
    "Applies rule-based functions that infer missing values directly from existing information:\n",
    "\n",
    "* `infer_missing_types_logic()` → infers `property_type` and `room_type` from the description text\n",
    "* `impute_availability_logic()` → imputes yearly availability from 90-day availability\n",
    "* `impute_neighbourhood_from_geo()` → restores neighbourhood logic using suburb and municipality\n",
    "\n",
    "**2. Text fallbacks**\n",
    "\n",
    "Fills missing text fields with clear placeholders such as *“Unnamed Listing”* or *“No description provided”*.\n",
    "\n",
    "**3. Learn statistics from the training dataset**\n",
    "\n",
    "Builds an imputation dictionary using training-only data:\n",
    "\n",
    "* Categorical modes (most frequent values)\n",
    "* Numerical replacements based on skew-aware logic\n",
    "\n",
    "  * If the distribution is symmetric → use mean\n",
    "  * If skewed → use median\n",
    "* Hierarchical mapping:\n",
    "  **property_type → most common room_type**\n",
    "\n",
    "**4. Apply learned imputations to both train and test**\n",
    "\n",
    "Uses the training statistics to fill missing values consistently:\n",
    "\n",
    "* Applies the learned categorical and numerical values\n",
    "* Fills room types using the property-type hierarchy\n",
    "* Falls back to global modes where needed\n",
    "\n",
    "**5. Internal date imputation**\n",
    "\n",
    "Sorts rows by `ID` and uses forward/backward fill to impute missing dates like `first_review` and `last_review` within each dataset independently.\n",
    "\n",
    "---\n",
    "This design prevents data leakage by learning imputations only from the training data, ensures consistent behaviour across both datasets, blends rule-based inference with statistical methods, and maintains a modular structure that aligns cleanly with the overall multi-stage pipeline.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39241d-1710-49cf-bae7-3a4603397b41",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0541c5-b348-46c6-b0c1-c1b9be160f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_amenities(train_df, test_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Learns top N amenities from TRAIN, creates binary columns in both.\n",
    "    \"\"\"\n",
    "    # 1. Analyze Train to get Top N\n",
    "    train_exploded = train_df.explode('amenities_list')\n",
    "    top_amenities = train_exploded['amenities_list'].value_counts().head(top_n).index.tolist()\n",
    "    \n",
    "    # 2. Apply to both\n",
    "    for df in [train_df, test_df]:\n",
    "        for amenity in top_amenities:\n",
    "            col_name = f'has_{amenity.replace(\" \", \"_\").replace(\"-\", \"_\")}'\n",
    "            # Check list column exists before applying\n",
    "            if 'amenities_list' in df.columns:\n",
    "                df[col_name] = df['amenities_list'].apply(lambda x: int(amenity in x))\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea0af9-431c-4a8c-9fd7-72263f55b9eb",
   "metadata": {},
   "source": [
    "**`encode_amenities(train_df, test_df)`**\n",
    "\n",
    "This function identifies the top N most common amenities from the training dataset and creates matching binary indicator columns in both datasets.\n",
    "\n",
    "1. It first explodes the amenities_list column in the training set and selects the top N most frequent amenities.\n",
    "\n",
    "2. For each of these amenities, it adds a new column such as has_wifi or has_heating, where values are 1 if the amenity is present and 0 otherwise.\n",
    "\n",
    "3. The same columns are then created in the test set to ensure the feature space is aligned across both datasets.\n",
    "\n",
    "Overall, this function converts unstructured amenity lists into structured that capture the popularity and presence of key amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7490fa1b-d89d-4cea-9ced-662e388a86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_property_type_encoding(df_train, df_test, price_col='price'):\n",
    "    \"\"\"\n",
    "    Advanced Property Type Encoding: Tiers, Grouping, and Target Encoding.\n",
    "    \"\"\"\n",
    "    # Avoid SettingWithCopy\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    # --- A. Price Tiers (for reference) ---\n",
    "    df_train['price_tier'] = pd.cut(\n",
    "        df_train[price_col], bins=[0, 100, 500, float('inf')],\n",
    "        labels=['Low', 'Mid', 'High'], right=False\n",
    "    )\n",
    "\n",
    "    # --- B. Tier Scores ---\n",
    "    # Learn high/mid lists from Train\n",
    "    high_tier_props = df_train[df_train['price_tier'] == 'High']['property_type'].value_counts().index.tolist()\n",
    "    mid_tier_props = df_train[df_train['price_tier'] == 'Mid']['property_type'].value_counts().index.tolist()\n",
    "\n",
    "    def compute_tier_score(prop_type):\n",
    "        if prop_type in high_tier_props[:10]: return 3\n",
    "        elif prop_type in mid_tier_props[:15]: return 2\n",
    "        elif prop_type in high_tier_props[10:20]: return 1.5\n",
    "        else: return 1\n",
    "\n",
    "    for df in [df_train, df_test]:\n",
    "        df['property_tier_score'] = df['property_type'].apply(compute_tier_score)\n",
    "\n",
    "    # --- C. Groupings ---\n",
    "    property_groups = {\n",
    "        'luxury_homes': ['Entire villa', 'Entire serviced apartment', 'Farm stay'],\n",
    "        'unique_stays': ['Entire cabin', 'Earthen home', 'Barn', 'Castle'],\n",
    "        'standard_rentals': ['Entire rental unit', 'Entire home', 'Private room in home'],\n",
    "        'budget_options': ['Shared room in rental unit', 'Shared room in hostel']\n",
    "    }\n",
    "    for group, props in property_groups.items():\n",
    "        for df in [df_train, df_test]:\n",
    "            df[f'is_{group}'] = df['property_type'].isin(props).astype(int)\n",
    "            df[f'{group}_tier'] = df[f'is_{group}'] * df['property_tier_score']\n",
    "\n",
    "    # --- D. Target Encoding ---\n",
    "    # Map tier to numeric for target encoding\n",
    "    target_mapping = {'High': 3, 'Mid': 2, 'Low': 1}\n",
    "    y_target = df_train['price_tier'].map(target_mapping).fillna(1) # Fill NaN with Low\n",
    "    \n",
    "    encoder = TargetEncoder(cols=['property_type'], smoothing=20)\n",
    "    df_train['property_type_encoded'] = encoder.fit_transform(df_train['property_type'], y_target)\n",
    "    df_test['property_type_encoded'] = encoder.transform(df_test['property_type'])\n",
    "\n",
    "    # --- E. Rare Premium Flag ---\n",
    "    rare_premium = df_train.groupby('property_type').filter(\n",
    "        lambda x: (x[price_col].mean() > 300) & (len(x) < 50)\n",
    "    )['property_type'].unique()\n",
    "\n",
    "    for df in [df_train, df_test]:\n",
    "        for pt in rare_premium:\n",
    "            col = f'is_{pt.replace(\" \", \"_\").replace(\"/\", \"_\").lower()}'\n",
    "            df[col] = (df['property_type'] == pt).astype(int)\n",
    "            \n",
    "    # Cleanup temporary column\n",
    "    df_train = df_train.drop(columns=['price_tier'])\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96afb0-601e-4f93-a246-89c51ea329cf",
   "metadata": {},
   "source": [
    "**`optimized_property_type_encoding(df_train, df_test)`**\n",
    "\n",
    "This function builds a rich set of features based on property_type, combining tiering, grouping, rare-category detection, and target encoding.\n",
    "\n",
    "1. It starts by categorizing each listing into price tiers (Low, Mid, High), learned from the training set.\n",
    "\n",
    "2. Based on these tiers, it learns which property types are most common in high-priced and mid-priced listings, creating a property_tier_score to reflect premium-level property types.\n",
    "\n",
    "3. It groups property types into meaningful categories (e.g., luxury homes, unique stays, standard rentals, budget options) and generates binary flags and weighted tier scores for each group.\n",
    "\n",
    "4. Target encoding is applied to property_type using price tiers, allowing the model to capture complex relationships between property type and pricing.\n",
    "\n",
    "5. It also detects “rare premium” property types (high average price but low frequency) and adds explicit flags for those categories.\n",
    "\n",
    "Overall, this function transforms raw property types into a comprehensive set of numerical and categorical indicators that help the model understand pricing signals associated with different property categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029d6299-cad8-4421-ab46-bfb7d0d7ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_neighbourhood_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Advanced Neighbourhood Encoding: Popularity Ratios, Premium Flags, Target Encoding.\n",
    "    \"\"\"\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "\n",
    "    # --- A. Setup Price Tiers (Train Only) ---\n",
    "    df_train['price_tier'] = pd.cut(\n",
    "        df_train['price'], bins=[0, 100, 500, float('inf')],\n",
    "        labels=['Low', 'Mid', 'High'], right=False\n",
    "    )\n",
    "\n",
    "    # --- B. Tier Popularity Scores ---\n",
    "    tier_popularity = df_train.groupby(['neighbourhood_cleansed', 'price_tier']).size().unstack()\n",
    "    tier_popularity = tier_popularity.div(tier_popularity.sum(axis=1), axis=0) # Normalize\n",
    "\n",
    "    for tier in ['High', 'Mid', 'Low']:\n",
    "        mapper = tier_popularity[tier].fillna(0)\n",
    "        df_train[f'neighbourhood_{tier.lower()}_score'] = df_train['neighbourhood_cleansed'].map(mapper)\n",
    "        df_test[f'neighbourhood_{tier.lower()}_score'] = df_test['neighbourhood_cleansed'].map(mapper).fillna(0)\n",
    "\n",
    "    # --- C. Premium & Rare Flags ---\n",
    "    high_ratio = tier_popularity['High'] / tier_popularity.sum(axis=1)\n",
    "    premium_neighs = high_ratio[high_ratio > 0.3].index.tolist()\n",
    "    \n",
    "    rare_neighs = df_train['neighbourhood_cleansed'].value_counts()\n",
    "    rare_list = rare_neighs[rare_neighs < 50].index\n",
    "\n",
    "    for df in [df_train, df_test]:\n",
    "        df['is_premium_neighbourhood'] = df['neighbourhood_cleansed'].isin(premium_neighs).astype(int)\n",
    "        df['is_rare_neighbourhood'] = df['neighbourhood_cleansed'].isin(rare_list).astype(int)\n",
    "\n",
    "    # --- D. Target Encoding ---\n",
    "    target_mapping = {'High': 3, 'Mid': 2, 'Low': 1}\n",
    "    y_target = df_train['price_tier'].map(target_mapping).fillna(1)\n",
    "\n",
    "    encoder = TargetEncoder(cols=['neighbourhood_cleansed'], smoothing=15)\n",
    "    df_train['neighbourhood_encoded'] = encoder.fit_transform(df_train['neighbourhood_cleansed'], y_target)\n",
    "    df_test['neighbourhood_encoded'] = encoder.transform(df_test['neighbourhood_cleansed'])\n",
    "\n",
    "    # Cleanup\n",
    "    df_train = df_train.drop(columns=['price_tier'])\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8011b7-77c5-4193-8929-6dfe391f98ed",
   "metadata": {},
   "source": [
    "**`encode_neighbourhood_features(df_train, df_test)`**\n",
    "\n",
    "This function creates advanced neighbourhood-level features by analyzing price tiers, popularity ratios, rarity, and encoded signals.\n",
    "\n",
    "1. It begins by assigning price tiers (Low, Mid, High) to each training listing and computes how frequently each neighbourhood appears in each price tier.\n",
    "\n",
    "2. These ratios are then used to create neighbourhood-level popularity scores, such as neighbourhood_high_score or neighbourhood_mid_score, for both train and test sets.\n",
    "\n",
    "3. The function identifies premium neighbourhoods (areas with a high proportion of expensive listings) and rare neighbourhoods (areas with very few listings), adding binary flags for both.\n",
    "\n",
    "4. Target encoding is applied to neighbourhood_cleansed, enabling the model to capture nuanced pricing effects tied to different local areas.\n",
    "\n",
    "Overall, this function enriches the dataset with detailed neighbourhood indicators, helping the model better recognise location-based pricing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8458a0f-89fb-40d1-b07c-a82e0b21fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_3_encoding(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Main pipeline to execute all encoding steps.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 3: Encoding Started ---\")\n",
    "    \n",
    "    # 1. Binary Columns (Map t/f to 1/0)\n",
    "    binary_cols = ['instant_bookable', 'host_is_superhost', 'host_has_profile_pic', \n",
    "                   'host_identity_verified', 'has_availability']\n",
    "    for df in [train_df, test_df]:\n",
    "        for col in binary_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].map({'t': 1, 'f': 0}).fillna(0).astype(int)\n",
    "\n",
    "    # 2. Ordinal Encoding (Response Time)\n",
    "    response_mapping = {\n",
    "        'a few days or more': 0, 'within a day': 1, \n",
    "        'within a few hours': 2, 'within an hour': 3\n",
    "    }\n",
    "    for df in [train_df, test_df]:\n",
    "        if 'host_response_time' in df.columns:\n",
    "            df['host_response_time_encoded'] = (\n",
    "                df['host_response_time'].map(response_mapping).fillna(-1).astype(int)\n",
    "            )\n",
    "\n",
    "    # 3. Amenities Encoding (Top N)\n",
    "    train_df, test_df = encode_amenities(train_df, test_df, top_n=10)\n",
    "\n",
    "    # 4. Host Location & Host Neighbourhood (Simple Top-K One Hot)\n",
    "    # We clean, keep top 5, and then One-Hot encode\n",
    "    def clean_loc(x):\n",
    "        return 'other' if (pd.isna(x) or ',' in str(x)) else x\n",
    "\n",
    "    for col, prefix in [('host_neighbourhood', 'neigh'), ('host_location', 'loc')]:\n",
    "        if col in train_df.columns:\n",
    "            # Clean\n",
    "            train_df[f'{col}_cleaned'] = train_df[col].apply(clean_loc)\n",
    "            test_df[f'{col}_cleaned'] = test_df[col].apply(clean_loc)\n",
    "            \n",
    "            # Identify Top 5 from TRAIN\n",
    "            top_5 = train_df[f'{col}_cleaned'].value_counts().nlargest(5).index.tolist()\n",
    "            \n",
    "            # Encode (Keep Top 5, else 'other')\n",
    "            for df in [train_df, test_df]:\n",
    "                df[f'{col}_encoded_label'] = df[f'{col}_cleaned'].apply(lambda x: x if x in top_5 else 'other')\n",
    "            \n",
    "            # One-Hot Encode (Get Dummies)\n",
    "            # Note: We do this separately to ensure alignment, then concat\n",
    "            train_dummies = pd.get_dummies(train_df[f'{col}_encoded_label'], prefix=prefix, drop_first=True)\n",
    "            test_dummies = pd.get_dummies(test_df[f'{col}_encoded_label'], prefix=prefix, drop_first=True)\n",
    "            \n",
    "            # Align columns (Ensure Test has same columns as Train)\n",
    "            test_dummies = test_dummies.reindex(columns=train_dummies.columns, fill_value=0)\n",
    "            \n",
    "            # Concat back\n",
    "            train_df = pd.concat([train_df, train_dummies.astype(int)], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies.astype(int)], axis=1)\n",
    "\n",
    "    # 5. Bathroom Type (One Hot)\n",
    "    if 'bath_type_cleaned' in train_df.columns:\n",
    "        train_dummies = pd.get_dummies(train_df['bath_type_cleaned'], prefix='bath_type')\n",
    "        test_dummies = pd.get_dummies(test_df['bath_type_cleaned'], prefix='bath_type')\n",
    "        \n",
    "        # Align columns\n",
    "        test_dummies = test_dummies.reindex(columns=train_dummies.columns, fill_value=0)\n",
    "        \n",
    "        train_df = pd.concat([train_df, train_dummies.astype(int)], axis=1)\n",
    "        test_df = pd.concat([test_df, test_dummies.astype(int)], axis=1)\n",
    "\n",
    "    # 6. Advanced Property & Neighbourhood Encoding\n",
    "    train_df, test_df = optimized_property_type_encoding(train_df, test_df)\n",
    "    train_df, test_df = encode_neighbourhood_features(train_df, test_df)\n",
    "\n",
    "    print(\"--- Stage 3: Encoding Complete ---\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fde48a-70ff-4046-8768-fa523835ae57",
   "metadata": {},
   "source": [
    "**`stage_3_encoding(train_df, test_df)`** \n",
    "\n",
    "This function applies all feature-encoding steps to transform cleaned and imputed data into model-ready numeric features for both the training and test datasets.\n",
    "\n",
    "1. First, it converts several binary text columns (`instant_bookable`, `host_is_superhost`, `host_has_profile_pic`, `host_identity_verified`, `has_availability`) from `'t'/'f'` strings into `1/0` integers, ensuring these yes/no fields are usable in numerical models.\n",
    "\n",
    "2. Next, it performs ordinal encoding on `host_response_time` by mapping categories such as *“a few days or more”*, *“within a day”*, *“within a few hours”*, and *“within an hour”* to ordered integer scores, so the model can understand the speed of host response as a ranked variable.\n",
    "\n",
    "3. It then calls `encode_amenities()` to learn the top N most common amenities from the training data and create corresponding binary features (e.g., `has_wifi`) in both train and test, turning the unstructured `amenities_list` into structured indicator variables.\n",
    "\n",
    "4. For `host_neighbourhood` and `host_location`, it cleans the values, identifies the top 5 most frequent categories from the training set, and one-hot encodes those while grouping all other values into an `\"other\"` category. It also aligns dummy columns between train and test so they share the same encoded structure.\n",
    "\n",
    "5. It applies one-hot encoding to `bath_type_cleaned`, creating separate columns for each bathroom type and ensuring that train and test have aligned bathroom-type dummy variables.\n",
    "\n",
    "* Finally, it calls `optimized_property_type_encoding()` and `encode_neighbourhood_features()` to generate advanced property-type and neighbourhood-based features, including tier scores, group flags, popularity ratios, premium/rare flags, and target-encoded variables.\n",
    "---\n",
    "Overall, this function centralises all encoding logic, ensuring that both the training and test datasets receive consistent, well-structured numeric features that are ready to be used by machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18655d-610b-4704-b6b4-b8bd9d21a2e5",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0afdda2-54fe-4c67-a860-502884ece9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2=-37.8136, lon2=144.9631):\n",
    "    \"\"\"Calculates distance (km) between lat/lon and Melbourne CBD.\"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    d_phi = np.radians(lat2 - lat1)\n",
    "    d_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(d_phi / 2) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(d_lambda / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bde50-f476-4fc8-8c6d-c3a3a554d78b",
   "metadata": {},
   "source": [
    "**`haversine_distance(lat1, lon1, lat2=-37.8136, lon2=144.9631)`** \n",
    "\n",
    "This function computes the geodesic (Haversine) distance in kilometres between a listing’s coordinates and the Melbourne CBD.\n",
    "\n",
    "1. It converts latitude and longitude values from degrees to radians.\n",
    "\n",
    "2. Applies the Haversine formula to account for Earth’s curvature.\n",
    "\n",
    "3. Returns the straight-line distance in kilometres, using Melbourne CBD (−37.8136, 144.9631) as the default reference point.\n",
    "\n",
    "Overall, this function generates a useful geographic feature that captures how far each listing is from a central, high-demand location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d318ba5-89bb-4dc9-acdf-9483339ac0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_score(text, analyzer):\n",
    "    \"\"\"Returns compound sentiment score (0 if missing).\"\"\"\n",
    "    if pd.isnull(text) or str(text).strip() == '':\n",
    "        return 0\n",
    "    return analyzer.polarity_scores(str(text))['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cda2e-a580-42d8-97f4-13882882ceeb",
   "metadata": {},
   "source": [
    "**`get_vader_score(text, analyzer)`** \n",
    "\n",
    "This function produces a VADER compound sentiment score for text fields such as listing descriptions or host bios.\n",
    "\n",
    "1. If the text is missing or empty, it returns 0 as a neutral baseline.\n",
    "\n",
    "2. Otherwise, it uses the VADER sentiment analyzer to compute the compound score, which ranges from −1 (very negative) to +1 (very positive).\n",
    "\n",
    "This allows the model to capture emotional tone or marketing quality in descriptions as a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0803b22c-b9ca-41e8-a9ee-4c255bd2315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_amenities_list(amenities_str, reference_list):\n",
    "    \"\"\"Counts how many reference items appear in the amenities string.\"\"\"\n",
    "    if pd.isna(amenities_str): return 0\n",
    "    clean_str = str(amenities_str).lower().replace('{', '').replace('}', '').replace('\"', '')\n",
    "    return sum(1 for item in reference_list if item.lower() in clean_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105685e-79be-46fa-8864-fb86a9c710d1",
   "metadata": {},
   "source": [
    "**`count_amenities_list(amenities_str, reference_list)`** \n",
    "\n",
    "This function counts how many items from a predefined reference list (e.g., “top 25 amenities”) appear in the raw amenities string.\n",
    "\n",
    "1. Handles missing values by returning 0.\n",
    "\n",
    "2. Cleans the raw amenities text by removing brackets and quotes.\n",
    "\n",
    "3. Checks each reference amenity against the cleaned string and counts how many are present.\n",
    "\n",
    "This helps quantify amenity richness in a simple numeric way, especially when extracting important amenities from messy text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e97efc5d-1d30-4b35-8d61-0f7547b1b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basic_structural_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- Review & Host Dates ---\n",
    "    review_cols = [\n",
    "        'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', \n",
    "        'review_scores_communication', 'review_scores_location', 'review_scores_value'\n",
    "    ]\n",
    "    # Calculate avg/min if columns exist\n",
    "    present_cols = [c for c in review_cols if c in df.columns]\n",
    "    if present_cols:\n",
    "        df['review_scores_avg'] = df[present_cols].mean(axis=1)\n",
    "        df['review_scores_min'] = df[present_cols].min(axis=1)\n",
    "\n",
    "    if 'number_of_reviews' in df.columns:\n",
    "        df['review_count_log'] = np.log1p(df['number_of_reviews'])\n",
    "\n",
    "    # Host Years & Days Since Last Review\n",
    "\n",
    "    # Ensure columns exist so later steps don't break\n",
    "    if 'host_since' not in df.columns:\n",
    "        df['host_since'] = pd.NaT\n",
    "    if 'last_review' not in df.columns:\n",
    "        df['last_review'] = pd.NaT\n",
    "\n",
    "    # Step 1: Convert date columns to datetime format\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'], errors='coerce')\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "\n",
    "    # Step 2: Use the latest date in the dataset as a reference point\n",
    "    reference_date = max(df['host_since'].max(), df['last_review'].max())\n",
    "\n",
    "    # Step 3: Calculate host experience in years\n",
    "    df['host_years'] = (reference_date - df['host_since']).dt.days / 365\n",
    "\n",
    "    # Step 4: Fill missing host_since values with 0 (no experience)\n",
    "    df['host_years'] = df['host_years'].fillna(0)\n",
    "\n",
    "    # --- Step 1: Convert 'last_review' to datetime ---\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "\n",
    "    # --- Step 2: Define reference date as the latest valid review date ---\n",
    "    reference_date = df['last_review'].max()\n",
    "\n",
    "    # --- Step 3: Calculate time since last review ---\n",
    "    df['days_since_last_review'] = (reference_date - df['last_review']).dt.days\n",
    "\n",
    "    # --- Step 4: Fill missing with max to represent \"never reviewed\" or very old ---\n",
    "    df['days_since_last_review'] = df['days_since_last_review'].fillna(\n",
    "        df['days_since_last_review'].max()\n",
    "    )\n",
    "\n",
    "    # Reapplied to reinforce datetime consistency and ensure no downstream overwrite\n",
    "    # corrupted the time-based feature; this keeps review-based features stable and reproducible.\n",
    "\n",
    "    # --- Step 1: Convert 'last_review' to datetime ---\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "\n",
    "    # --- Step 2: Define reference date as the latest valid review date ---\n",
    "    reference_date = df['last_review'].max()\n",
    "\n",
    "    # --- Step 3: Calculate time since last review ---\n",
    "    df['days_since_last_review'] = (reference_date - df['last_review']).dt.days\n",
    "\n",
    "    # --- Step 4: Fill missing with max to represent \"never reviewed\" or very old ---\n",
    "    df['days_since_last_review'] = df['days_since_last_review'].fillna(\n",
    "        df['days_since_last_review'].max()\n",
    "    )\n",
    "\n",
    "    # --- Property Structure ---\n",
    "    df['is_entire_home'] = (df['room_type'] == 'Entire home/apt').astype(int)\n",
    "    df['is_hotel_room'] = df['room_type'].astype(str).str.lower().str.contains('hotel').astype(int)\n",
    "    \n",
    "    if 'bedrooms' in df.columns:\n",
    "        df['entire_home_x_bedrooms'] = df['is_entire_home'] * df['bedrooms']\n",
    "\n",
    "    if 'beds' in df.columns and 'accommodates' in df.columns:\n",
    "        df['beds_per_guest'] = np.log1p(df['beds'] / df['accommodates'].replace(0, 1))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818069f-56ba-4baa-87ff-d1e6515d1c8a",
   "metadata": {},
   "source": [
    "**`add_basic_structural_features(df)`**\n",
    "\n",
    "This function engineers core structural features about reviews, host experience, and listing configuration.\n",
    "\n",
    "1. It first looks for individual review score columns (accuracy, cleanliness, check-in, communication, location, value). If they exist, it summarises them into two aggregate metrics:\n",
    "\n",
    "  * `review_scores_avg` – the average of available review scores per listing\n",
    "  * `review_scores_min` – the minimum review score per listing, capturing the weakest area of guest feedback.\n",
    "\n",
    "2. If `number_of_reviews` is present, it creates `review_count_log` using a log(1 + x) transformation to stabilise the skew often seen in review counts while preserving the relative ordering between listings.\n",
    "\n",
    "3. For host experience and review recency, it first ensures `host_since` and `last_review` exist, converts them to datetime, and uses the latest available date in the dataset as a reference point to compute `host_years`, which measures how long the host has been active in years, filling missing values with 0 to represent no experience.\n",
    "\n",
    "4. It then computes `days_since_last_review` as the number of days since the most recent valid review date, filling missing values with the maximum observed value to represent listings that have never been reviewed or have very old reviews. This calculation is intentionally reapplied to reinforce datetime consistency and protect this time-based feature from any accidental overwrites in later steps.\n",
    "\n",
    "5. Finally, it adds structure-related features:\n",
    "\n",
    "  * `is_entire_home` – a flag for listings where `room_type` is “Entire home/apt”\n",
    "  * `is_hotel_room` – a flag for listings whose room type text contains “hotel”\n",
    "  * `entire_home_x_bedrooms` – an interaction capturing how many bedrooms entire homes have\n",
    "  * `beds_per_guest` – a log-transformed beds-per-accommodated-guest ratio, measuring how generous the sleeping setup is relative to capacity.\n",
    "\n",
    "Overall, this function compresses raw review, host, and structural information into clean, numeric features that are highly informative for downstream price modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac8ea9b-0190-4f34-845d-509e27e9e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- 1. Word/Char Counts  ---\n",
    "    if 'description' in df.columns:\n",
    "        df['desc_word_count'] = df['description'].astype(str).apply(lambda x: len(x.split()))\n",
    "        df['desc_char_count'] = df['description'].astype(str).apply(len)\n",
    "    \n",
    "    if 'host_about' in df.columns:\n",
    "        df['host_about_word_count'] = df['host_about'].astype(str).apply(lambda x: len(x.split()))\n",
    "        \n",
    "    if 'neighborhood_overview' in df.columns:\n",
    "        df['neigh_word_count'] = df['neighborhood_overview'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "    # --- 2. Sentiment (VADER) ---\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['desc_sentiment'] = df['description'].astype(str).apply(lambda x: get_vader_score(x, analyzer))\n",
    "    df['host_sentiment'] = df['host_about'].astype(str).apply(lambda x: get_vader_score(x, analyzer))\n",
    "    df['neigh_sentiment'] = df['neighborhood_overview'].astype(str).apply(lambda x: get_vader_score(x, analyzer))\n",
    "\n",
    "    # --- 3. Luxury Keywords (Full List) ---\n",
    "    luxury_keywords = [\n",
    "        'luxury', 'executive', 'beachfront', 'renovated', 'designer', 'stunning', 'premium',\n",
    "        'elegant', 'spacious', 'exclusive', 'high-end', 'retreat', 'resort', 'deluxe', \n",
    "        'panoramic', 'modern', 'gourmet', 'spa', 'oceanview', 'sunset view', \n",
    "        'chic', 'newly built', 'private pool', 'highrise', 'skyline view', 'penthouse', \n",
    "        'architect-designed', 'boutique', 'five-star', 'platinum', 'expansive'\n",
    "    ]\n",
    "    \n",
    "    # Regex cleaning exactly as requested\n",
    "    df['combined_text'] = (df['name'].astype(str) + ' ' + df['description'].astype(str)).str.lower()\n",
    "    df['combined_text'] = df['combined_text'].apply(lambda x: re.sub(r'[^a-z0-9\\s\\-]', ' ', x))\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    keyword_hits = []\n",
    "    for kw in luxury_keywords:\n",
    "        pattern = rf'\\b{re.escape(kw)}\\b'\n",
    "        col = f'has_kw_{kw.replace(\" \", \"_\")}'\n",
    "        df[col] = df['combined_text'].str.contains(pattern, regex=True).astype(int)\n",
    "        keyword_hits.append(df[col])\n",
    "\n",
    "    df['luxury_keyword_hits'] = sum(keyword_hits)\n",
    "    df = df.drop(columns=['combined_text']) # Cleanup\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3532e-9fd4-4bdd-ac22-ffc50b6cce02",
   "metadata": {},
   "source": [
    "**`add_text_features(df)`**\n",
    "\n",
    "This function extracts length, sentiment, and luxury-related signals from listing text fields.\n",
    "\n",
    "1. For `description`, `host_about`, and `neighborhood_overview`, it creates word-count and character-count style metrics such as `desc_word_count`, `desc_char_count`, `host_about_word_count`, and `neigh_word_count`, which capture how detailed each text field is.\n",
    "\n",
    "2. It then uses VADER to compute sentiment scores for descriptions, host bios, and neighbourhood overviews (`desc_sentiment`, `host_sentiment`, `neigh_sentiment`), providing a numeric measure of how positive or negative the text reads.\n",
    "\n",
    "3. It builds a `combined_text` field from `name` and `description`, cleans it with regex, and then scans for a curated list of “luxury” keywords (e.g., *luxury*, *executive*, *penthouse*, *private pool*). For each keyword, it creates a binary flag like `has_kw_penthouse`.\n",
    "\n",
    "4. Finally, it aggregates all these keyword flags into a single `luxury_keyword_hits` feature that counts how many luxury terms appear, and drops the temporary `combined_text` column.\n",
    "\n",
    "Overall, this function turns unstructured text into length, sentiment, and luxury-signal features that help the model understand how listings are presented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9479f54-01c4-471d-a30d-19251456ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_amenity_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- Full Reference Lists ---\n",
    "    unique_amenities = [\n",
    "        'airport transfer', 'private pool', 'pool table', 'piano', 'private hot tub', 'waterfront',\n",
    "        'mountain view', 'bay view', 'ocean view', 'sea view', 'marina view', 'valley view',\n",
    "        'vineyard view', 'pool view', 'lake view', 'beach view', 'river view', 'harbor view'\n",
    "    ]\n",
    "    luxury_amenities = [\n",
    "        'dishwasher', 'private entrance', 'private patio or balcony', 'elevator', 'outdoor furniture',\n",
    "        'bathtub', 'high chair', 'crib', 'bbq grill', 'pets allowed', 'gym',\n",
    "        'private backyard – fully fenced', 'city skyline view', 'indoor fireplace', 'pool',\n",
    "        'netflix', 'hot tub', 'heated', 'yoga mat',\n",
    "        'free parking garage on premises – 1 space',\n",
    "        'free residential garage on premises – 1 space', 'coffee maker'\n",
    "    ]\n",
    "    standard_amenities = [\n",
    "        'essentials', 'smoke alarm', 'kitchen', 'hangers', 'hair dryer', 'iron',\n",
    "        'wifi', 'hot water', 'heating', 'microwave', 'dishes and silverware', 'shampoo',\n",
    "        'refrigerator', 'cooking basics', 'bed linens', 'tv', 'air conditioning',\n",
    "        'free parking on premises', 'washer', 'stove', 'resort access', 'ev charger'\n",
    "    ]\n",
    "\n",
    "    # --- Counts & Ratios ---\n",
    "    df['count_unique_amenities'] = df['amenities'].apply(lambda x: count_amenities_list(x, unique_amenities))\n",
    "    df['count_luxury_amenities'] = df['amenities'].apply(lambda x: count_amenities_list(x, luxury_amenities))\n",
    "    df['count_standard_amenities'] = df['amenities'].apply(lambda x: count_amenities_list(x, standard_amenities))\n",
    "\n",
    "    # Total Amenities\n",
    "    if 'amenities_list' in df.columns:\n",
    "        df['total_amenities'] = df['amenities_list'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    else:\n",
    "        df['total_amenities'] = df['amenities'].astype(str).str.count(',') + 1\n",
    "        \n",
    "    df['luxury_ratio'] = df['count_luxury_amenities'] / (df['total_amenities'] + 1e-6)\n",
    "    df['standard_ratio'] = df['count_standard_amenities'] / (df['total_amenities'] + 1e-6)\n",
    "\n",
    "    # --- Premium Flags (Full Mapping) ---\n",
    "    premium_flags = {\n",
    "        'has_sauna': 'sauna',\n",
    "        'has_dishwasher': 'dishwasher',\n",
    "        'has_private_patio': 'private patio or balcony',\n",
    "        'has_bathtub': 'bathtub',\n",
    "        'has_dryer': 'dryer',\n",
    "        'has_pool': 'pool',\n",
    "        'has_gym': 'gym',\n",
    "        'has_fireplace': 'indoor fireplace',\n",
    "        'has_city_view': 'city skyline view',\n",
    "        'has_mountain_view': 'mountain view',\n",
    "        'has_waterfront': 'waterfront',\n",
    "        'has_refrigerator':'refrigerator'\n",
    "    }\n",
    "    for col, kw in premium_flags.items():\n",
    "        df[col] = df['amenities'].astype(str).str.lower().str.contains(kw.lower(), regex=False).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285a13d-a0ea-440b-bc25-96462f51b65a",
   "metadata": {},
   "source": [
    "**`add_amenity_features(df)`**\n",
    "\n",
    "This function builds detailed amenity-based features using curated lists of unique, luxury, and standard amenities.\n",
    "\n",
    "1. It defines three reference lists: `unique_amenities` (e.g., waterfront, mountain view), `luxury_amenities` (e.g., private entrance, gym, hot tub), and `standard_amenities` (e.g., wifi, kitchen, heating).\n",
    "\n",
    "2. Using `count_amenities_list()`, it creates `count_unique_amenities`, `count_luxury_amenities`, and `count_standard_amenities`, which measure how many amenities from each group a listing has.\n",
    "\n",
    "3. It then calculates `total_amenities` either from `amenities_list` (preferred) or by counting commas in the raw `amenities` string, and derives `luxury_ratio` and `standard_ratio` by dividing the relevant counts by the total amenities.\n",
    "\n",
    "\n",
    "4. It adds a set of `premium_flags` (e.g., `has_sauna`, `has_dishwasher`, `has_private_patio`, `has_pool`, `has_city_view`, `has_waterfront`, `has_refrigerator`) by checking if those amenity terms appear in the amenities text.\n",
    "\n",
    "Overall, this function converts raw amenity strings into rich numerical and binary indicators that capture both quantity and quality of amenities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a50c4a4-0005-4a38-8287-16bc7f1b73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geolocation_simple(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        df['distance_to_cbd_km'] = haversine_distance(df['latitude'], df['longitude'])\n",
    "        df['inv_distance_to_cbd'] = 1 / (1 + df['distance_to_cbd_km'])\n",
    "        df['is_cbd'] = (df['distance_to_cbd_km'] <= 2).astype(int)\n",
    "        \n",
    "        # Zones\n",
    "        bins = [0, 3, 7, 15, np.inf]\n",
    "        labels = ['central', 'inner', 'middle', 'outer']\n",
    "        df['distance_zone'] = pd.cut(df['distance_to_cbd_km'], bins=bins, labels=labels, right=False)\n",
    "        \n",
    "        # One-Hot Encode Zones\n",
    "        zone_dummies = pd.get_dummies(df['distance_zone'], prefix='dist_zone',dtype=int)\n",
    "        df = pd.concat([df, zone_dummies], axis=1)\n",
    "        \n",
    "        # Interactions\n",
    "        df['dist_entire_home'] = df['distance_to_cbd_km'] * df['is_entire_home']\n",
    "        if 'dist_zone_central' in df.columns:\n",
    "            df['entire_home_central'] = ((df['is_entire_home'] == 1) & (df['dist_zone_central'] == 1)).astype(int)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2062748-6c58-4435-a038-60476411543f",
   "metadata": {},
   "source": [
    "**`add_geolocation_simple(df)`**\n",
    "\n",
    "This function creates distance-based features from the listing’s coordinates relative to Melbourne CBD.\n",
    "\n",
    "1. If `latitude` and `longitude` are available, it computes `distance_to_cbd_km` using the Haversine function and then derives `inv_distance_to_cbd` as an inverse-distance feature (closer listings get higher scores).\n",
    "\n",
    "2. It flags `is_cbd` for listings within 2 km of the CBD.\n",
    "\n",
    "3. It assigns each listing to a `distance_zone` (central, inner, middle, outer) based on distance bands and then one-hot encodes these zones into dummy variables like `dist_zone_central`.\n",
    "\n",
    "4. It also creates interaction features such as `dist_entire_home` (distance × entire-home flag) and `entire_home_central` (entire home located in the central zone).\n",
    "\n",
    "Overall, this function summarises location into interpretable distance zones and interactions that help the model capture how proximity to the CBD affects price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "984c309f-05a6-4af1-a229-31c4fee56833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_osm_transit_features(df):\n",
    "    \"\"\"\n",
    "    Uses OSMnx to fetch Melbourne transit stops and calculate distance/count.\n",
    "    WARNING: Requires Internet and runs somewhat slowly.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Fetch Stops\n",
    "    tags = {\"railway\": [\"station\", \"tram_stop\", \"subway_entrance\"], \"public_transport\": \"platform\"}\n",
    "    transit_stops = ox.features_from_place(\"Melbourne, Australia\", tags=tags)\n",
    "    transit_stops = transit_stops[transit_stops.geometry.type == \"Point\"]\n",
    "    gdf_stops = transit_stops[['geometry']].copy().to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # 2. Convert Listings to GDF\n",
    "    gdf_listings = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\"\n",
    "    )\n",
    "    \n",
    "    # 3. Distance (BallTree)\n",
    "    listing_coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "    stop_coords = np.radians(np.array([[p.y, p.x] for p in gdf_stops.geometry]))\n",
    "    \n",
    "    tree = BallTree(stop_coords, metric='haversine')\n",
    "    distances, _ = tree.query(listing_coords, k=1)\n",
    "    df['min_transit_dist_km'] = distances.flatten() * 6371\n",
    "    \n",
    "    # 4. Count within 500m\n",
    "    gdf_listings['geometry'] = gdf_listings.buffer(500 / 111320)\n",
    "    \n",
    "    join = gpd.sjoin(gdf_stops, gdf_listings[['ID', 'geometry']], how='inner', predicate='within')\n",
    "    counts = join.groupby('ID').size().reset_index(name='num_transit_500m')\n",
    "    \n",
    "    df = df.merge(counts, on='ID', how='left')\n",
    "    df['num_transit_500m'] = df['num_transit_500m'].fillna(0).astype(int)\n",
    "    \n",
    "    df['transit_score'] = df['num_transit_500m'] / (1 + df['min_transit_dist_km'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ade0eb-9892-4040-926f-8273b5e4ec21",
   "metadata": {},
   "source": [
    "**`add_osm_transit_features(df)`**\n",
    "\n",
    "This function uses OpenStreetMap (OSM) data to measure public transport accessibility around each listing.\n",
    "\n",
    "1. It fetches transit-related points (stations, tram stops, platforms, etc.) in Melbourne using OSMnx and keeps only point geometries.\n",
    "   \n",
    "2. It converts listings to a GeoDataFrame and uses a BallTree (with haversine distance) to compute `min_transit_dist_km`, the distance from each listing to the closest transit stop.\n",
    "\n",
    "3. It buffers each listing by roughly 500 metres, performs a spatial join with transit stops, and counts how many transit stops fall within that buffer, storing the result in `num_transit_500m`.\n",
    "\n",
    "4. It then creates a `transit_score` that combines both density and distance: `num_transit_500m / (1 + min_transit_dist_km)`.\n",
    "\n",
    "Overall, this function adds external geospatial context by quantifying how well each listing is served by public transport, which can be a strong driver of price and attractiveness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90659505-e808-423a-b490-4b79ef24e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_4_engineering(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Master function to apply all feature engineering steps.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 4: Feature Engineering Started ---\")\n",
    "    \n",
    "    # --- A. INDEPENDENT FEATURES (Apply to both separately) ---\n",
    "    def apply_independent(df):\n",
    "        df = add_basic_structural_features(df)\n",
    "        df = add_text_features(df)\n",
    "        df = add_amenity_features(df)\n",
    "        df = add_geolocation_simple(df)\n",
    "        \n",
    "        # OPTIONAL: Run OSMnx (Heavy computation)\n",
    "        # Only run if lat/lon exists. Handles errors internally.\n",
    "        if 'latitude' in df.columns:\n",
    "            df = add_osm_transit_features(df)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    train_df = apply_independent(train_df)\n",
    "    \n",
    "    test_df = apply_independent(test_df)\n",
    "    \n",
    "    # --- B. DEPENDENT FEATURES (Train -> Test Leakage Prevention) ---\n",
    "    \n",
    "    # Neighborhood Density (Calculate on Train, Map to Test)\n",
    "    neigh_density = train_df['neighbourhood_cleansed'].value_counts(normalize=True)\n",
    "    \n",
    "    train_df['neigh_density'] = train_df['neighbourhood_cleansed'].map(neigh_density)\n",
    "    test_df['neigh_density'] = test_df['neighbourhood_cleansed'].map(neigh_density).fillna(0)\n",
    "    \n",
    "    print(\"--- Stage 4: Feature Engineering Complete ---\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ea96f-40ec-48db-8209-ae937125754d",
   "metadata": {},
   "source": [
    "**`stage_4_engineering(train_df, test_df)`**\n",
    "\n",
    "This function is the master controller for all feature engineering and constructs the final model-ready feature set for both the training and test datasets.\n",
    "\n",
    "1. It first defines an internal helper, `apply_independent(df)`, which applies several independent feature blocks to each dataset separately:\n",
    "- `add_basic_structural_features()` (reviews, host experience, structure),\n",
    "- `add_text_features()` (length, sentiment, luxury keywords),\n",
    "- `add_amenity_features()` (counts, ratios, premium amenity flags), and\n",
    "- `add_geolocation_simple()` (distance to CBD, zones, interactions).\n",
    "-  If latitude and longitude are available, it also optionally runs `add_osm_transit_features()` to include public transport accessibility features.\n",
    "\n",
    "2. This independent pipeline is applied to `train_df` and `test_df` separately so that each dataset gets the same feature logic without cross-contamination.\n",
    "\n",
    "3. After that, it creates a dependent feature, `neigh_density`, by computing the relative frequency of each `neighbourhood_cleansed` in the **training data only**, and then mapping these density values onto both train and test. Any neighbourhoods not seen in training receive a default density of 0 in the test set.\n",
    "\n",
    "---\n",
    "Overall, this function brings together all Stage 4 feature engineering steps, ensuring that both train and test are enriched with consistent, information-rich features while still preventing data leakage by learning neighbourhood-level statistics only from the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a7c00-7a28-47bb-a95d-f76b734f2ec1",
   "metadata": {},
   "source": [
    "## Data Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3730c0ca-e96c-4b2a-bfbb-2973e6cf8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_caps(df, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    caps = {}\n",
    "    \n",
    "    # Logic is safe: It internally selects only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'int32']).columns\n",
    "    cols_to_process = [c for c in numeric_cols if c != 'ID'] \n",
    "    \n",
    "    for col in cols_to_process:\n",
    "        lower_bound = df[col].quantile(lower_quantile)\n",
    "        upper_bound = df[col].quantile(upper_quantile)\n",
    "        caps[col] = (lower_bound, upper_bound)\n",
    "        \n",
    "    return caps\n",
    "\n",
    "def apply_capping(df, caps):\n",
    "    df = df.copy()\n",
    "    for col, (lower, upper) in caps.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081536f-89df-4fa8-8f64-d8ff65e0d397",
   "metadata": {},
   "source": [
    "**get_outlier_caps(df, lower_quantile=0.01, upper_quantile=0.99)**\n",
    "\n",
    "This function calculates outlier capping thresholds for each numeric column in the dataset.\n",
    "\n",
    "* It automatically selects only numeric columns (excluding the `ID` column).\n",
    "* For each numeric variable, it computes the lower and upper quantile values—by default the 1st and 99th percentiles.\n",
    "* These quantile-based bounds are stored in a dictionary as `(lower_bound, upper_bound)` pairs for every column.\n",
    "\n",
    "Overall, this function learns the capping thresholds needed to reduce the influence of extreme outliers while preserving the majority of the data’s distribution.\n",
    "\n",
    "---\n",
    " **apply_capping(df, caps)**\n",
    "\n",
    "This function applies quantile-based outlier clipping to a dataset using pre-computed caps.\n",
    "\n",
    "* It iterates through each column listed in the `caps` dictionary.\n",
    "* If the column exists in the dataset, its values are clipped so that anything below the lower bound is set to that bound, and anything above the upper bound is capped at the upper bound.\n",
    "* A copy of the dataset is returned to avoid modifying the original.\n",
    "\n",
    "Overall, this function enforces the learned outlier caps on any dataset (train or test), ensuring consistent outlier handling across both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aceff810-5e8b-488f-9840-5e104af9dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_5_transformation(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Main Orchestrator for Transformation (Outlier Capping).\n",
    "    Learns caps from TRAIN numeric data, applies to both.\n",
    "    \"\"\"\n",
    "    print(\"--- Stage 5: Transformation (Outlier Capping) Started ---\")\n",
    "    \n",
    "    # 1. Learn Caps from Training Data\n",
    "    # (The helper function will automatically pick out the numeric columns)\n",
    "    outlier_caps = get_outlier_caps(train_df, lower_quantile=0.01, upper_quantile=0.99)\n",
    "    \n",
    "    # 2. Apply Caps to Training Data\n",
    "    # (We pass the FULL dataframe. Only the specific numeric columns will be modified)\n",
    "    train_transformed = apply_capping(train_df, outlier_caps)\n",
    "    \n",
    "    # 3. Apply Caps to Test Data\n",
    "    test_transformed = apply_capping(test_df, outlier_caps)\n",
    "    \n",
    "    print(\"--- Stage 5: Transformation Complete ---\")\n",
    "    return train_transformed, test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8c784-2871-41fc-ac33-3daaf4836812",
   "metadata": {},
   "source": [
    "**stage_5_transformation(train_df, test_df)**\n",
    "\n",
    "This function performs the final transformation step by applying quantile-based outlier capping in a way that prevents data leakage and keeps both datasets consistent.\n",
    "\n",
    "1. It begins by using `get_outlier_caps()` to learn the lower and upper quantile thresholds for every numeric column **from the training dataset only**, ensuring that test data does not influence the learned caps.\n",
    "\n",
    "2. It then applies these learned caps to the training data using `apply_capping()`, clipping extreme values to within the 1st and 99th percentile range for each column.\n",
    "\n",
    "3. The same caps are applied to the test data, guaranteeing that both datasets are treated identically, even if their distributions differ.\n",
    "---\n",
    "\n",
    "Overall, this function enforces consistent outlier handling across train and test while preventing leakage by learning all capping thresholds exclusively from the training dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eacdb0d-df53-4468-ba30-8970eb551bf1",
   "metadata": {},
   "source": [
    "## Data Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dddf182-c3f8-4a86-bafa-39fe55ef7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_6_finalise_and_export(df, filename):\n",
    "    \"\"\"\n",
    "    Final cleanup before saving:\n",
    "    - Drops intermediate list/text columns that can't be used in modeling.\n",
    "    - Saves to CSV.\n",
    "    \"\"\"\n",
    "    # Columns to drop because they are raw text, lists, or intermediate helpers\n",
    "    cols_to_drop = [\n",
    "        # Raw Text (Use only the extracted numeric features for modeling)\n",
    "        'description', 'neighborhood_overview', 'host_about', 'host_verifications', \n",
    "        'amenities', 'name', 'source','host_name','host_since','host_location','host_response_time',\n",
    "        'host_neighbourhood','neighbourhood','neighbourhood_cleansed','property_type',\n",
    "        'room_type','first_review','last_review', 'suburb','municipality',\n",
    "        \n",
    "        # Lists (not compatible with standard CSV/Models)\n",
    "        'amenities_list', 'verifications_list', \n",
    "        \n",
    "        # Intermediate/Helper Columns\n",
    "        'bath_type', 'bath_type_cleaned', 'host_neighbourhood_cleaned', 'host_location_cleaned',\n",
    "        'host_neighbourhood_encoded_label', 'host_location_encoded_label',\n",
    "        'distance_zone','price_tier'\n",
    "    ]\n",
    "    \n",
    "    # Drop only what exists in the dataframe\n",
    "    existing_drops = [c for c in cols_to_drop if c in df.columns]\n",
    "    df_final = df.drop(columns=existing_drops)\n",
    "    \n",
    "    # Save\n",
    "    df_final.to_csv(filename, index=False)\n",
    "    print(f\"   > Saved {filename} with shape {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87ada7-7f97-4f10-9083-4f47d63d06d0",
   "metadata": {},
   "source": [
    "**`step_6_finalise_and_export(df, filename)`**\n",
    "\n",
    "This function performs the final cleanup before exporting the engineered dataset. \n",
    "1. It removes all columns that are unsuitable for modelling—such as raw text fields, list-like structures, and intermediate helper variables that were only needed during earlier pipeline stages.\n",
    "\n",
    "2. After dropping only the columns that actually exist in the dataframe, it saves the cleaned dataset to a CSV file and prints the final shape, ensuring that only the refined, model-ready features remain in the exported output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8baf309-8eec-4bdc-a207-41ce0cbb1c9d",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c946e26f-b75d-4fe6-9469-25f0eb66ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PIPELINE EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "def preprocess_pipeline(train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                        lookup_csv_path: str = \"suburb_municipality_lookup.csv\"):\n",
    "    \"\"\"\n",
    "    Orchestrates the full preprocessing pipeline using current stages:\n",
    "      - Stage 1: Cleaning\n",
    "      - Stage 2: Imputation\n",
    "      - Stage 3: Feature Engineering\n",
    "      - Stage 4: Encoding\n",
    "      - Stage 5: Transformation (Outlier Capping)\n",
    "    Returns:\n",
    "      processed_train_df, processed_test_df\n",
    "    \"\"\"\n",
    "\n",
    "    # --- STAGE 1: CLEANING ---\n",
    "    print(\"=== Stage 1: Cleaning (Formatting & Types) ===\")\n",
    "    train_df = step_1_cleaning(train_df, lookup_csv_path=lookup_csv_path)\n",
    "    test_df  = step_1_cleaning(test_df,  lookup_csv_path=lookup_csv_path)\n",
    "\n",
    "    # --- STAGE 2: IMPUTATION ---\n",
    "    # stage_2_imputation currently returns (train_filled, test_filled)\n",
    "    train_df, test_df = stage_2_imputation(train_df, test_df)\n",
    "\n",
    "    # --- STAGE 3: ENCODING ---\n",
    "    # (Semantically Stage 3, function name = stage_3_encoding)\n",
    "    train_df, test_df = stage_3_encoding(train_df, test_df)\n",
    "\n",
    "    # --- STAGE 4: FEATURE ENGINEERING ---\n",
    "    # (Semantically Stage 4, function name = stage_4_engineering)\n",
    "    train_df, test_df = stage_4_engineering(train_df, test_df)\n",
    "\n",
    "    # --- STAGE 5: TRANSFORMATION (OUTLIER CAPPING) ---\n",
    "    train_df, test_df = stage_5_transformation(train_df, test_df)\n",
    "\n",
    "    print(\"=== Pipeline: All Stages Complete ===\")\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a2b16-e911-41ae-a422-c43361a40a3e",
   "metadata": {},
   "source": [
    "**`preprocess_pipeline(train_df, test_df, lookup_csv_path)`**\n",
    "\n",
    "This function orchestrates the entire preprocessing workflow by running all stages in sequence on both the training and test datasets.\n",
    "\n",
    "* **Stage 1 – Cleaning:**\n",
    "  Calls `step_1_cleaning()` to standardise formats, parse dates, clean text, process amenities and verifications, and merge geolocation lookup information for both train and test.\n",
    "\n",
    "* **Stage 2 – Imputation:**\n",
    "  Uses `stage_2_imputation()` to first apply rule-based, row-level fixes, then learn imputation statistics (modes, means/medians, hierarchical mappings) from the **training data only**, and finally apply consistent imputations to both datasets.\n",
    "\n",
    "* **Stage 3 – Encoding:**\n",
    "  Runs `stage_3_encoding()` to convert binary flags, ordinal response time, amenities, host location/neighbourhood, bathroom type, property type, and neighbourhood fields into aligned numeric and one-hot encoded features across train and test.\n",
    "\n",
    "* **Stage 4 – Feature Engineering:**\n",
    "  Applies `stage_4_engineering()` to add higher-level features, including structural features (reviews, host experience, beds per guest), text-based features (length, sentiment, luxury keywords), amenity richness, distance to CBD, distance zones, transit accessibility, and neighbourhood density.\n",
    "\n",
    "* **Stage 5 – Transformation (Outlier Capping):**\n",
    "  Uses `stage_5_transformation()` to learn quantile-based outlier caps from the training data and apply the same caps to both train and test, ensuring consistent outlier handling without leakage.\n",
    "\n",
    "At the end, the function returns the fully processed `train_df` and `test_df`, ready for modelling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b4d2b33-58d6-4428-8eac-1f69ba2039a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datasets loaded: train.csv, test.csv\n",
      "   Train shape: (7000, 61)\n",
      "   Test  shape: (3000, 60)\n",
      "=== Stage 1: Cleaning (Formatting & Types) ===\n",
      "--- Stage 2: Processing missing values ---\n",
      "--- Stage 2: Imputation Complete ---\n",
      "--- Stage 3: Encoding Started ---\n",
      "--- Stage 3: Encoding Complete ---\n",
      "--- Stage 4: Feature Engineering Started ---\n",
      "--- Stage 4: Feature Engineering Complete ---\n",
      "--- Stage 5: Transformation (Outlier Capping) Started ---\n",
      "--- Stage 5: Transformation Complete ---\n",
      "=== Pipeline: All Stages Complete ===\n",
      "   > Saved train_cleaned.csv with shape (7000, 177)\n",
      "   > Saved test_cleaned.csv with shape (3000, 176)\n",
      " Pipeline complete. Files 'train_cleaned.csv' and 'test_cleaned.csv' are ready for modelling.\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"\n",
    "    Loads datasets, runs the full pipeline, and exports cleaned data\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load Raw Data\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df  = pd.read_csv('test.csv')\n",
    "    print(\" Datasets loaded: train.csv, test.csv\")\n",
    "    print(f\"   Train shape: {train_df.shape}\")\n",
    "    print(f\"   Test  shape: {test_df.shape}\")\n",
    "\n",
    "    # 2. Run Full Preprocessing Pipeline\n",
    "    train_cleaned, test_cleaned = preprocess_pipeline(train_df, test_df)\n",
    "\n",
    "    # 3. Export Final Clean Files\n",
    "    step_6_finalise_and_export(train_cleaned, 'train_cleaned.csv')\n",
    "    step_6_finalise_and_export(test_cleaned,  'test_cleaned.csv')\n",
    "\n",
    "    print(\" Pipeline complete. Files 'train_cleaned.csv' and 'test_cleaned.csv' are ready for modelling.\")\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27294b5f-b964-4b4f-958e-c8428c72d0fc",
   "metadata": {},
   "source": [
    "**`run_pipeline()`**\n",
    "\n",
    "This function serves as the end-to-end executor for entire data-preprocessing workflow.\n",
    "\n",
    "* **Step 1 – Load raw data:**\n",
    "  Reads the original `train.csv` and `test.csv` files into dataframes and prints their shapes so it can quickly verify that the correct data has been loaded.\n",
    "\n",
    "* **Step 2 – Run the full preprocessing pipeline:**\n",
    "  Passes both dataframes to `preprocess_pipeline()`, which handles all stages of cleaning, imputation, encoding, feature engineering, and transformation, returning clean, model-ready datasets.\n",
    "\n",
    "* **Step 3 – Export processed datasets:**\n",
    "  Uses `step_6_finalise_and_export()` to remove intermediate text/list columns and save the final feature sets as `train_cleaned.csv` and `test_cleaned.csv`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
